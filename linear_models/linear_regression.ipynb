{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "In linear regression we want to model the relationship between a **scalar dependent variable** $y$ and one or more **independent (predictor) variables** $\\boldsymbol{x}$.\n",
    "\n",
    "**Given:** \n",
    "- dataset $\\{(\\boldsymbol{x}^{(1)}, y^{(1)}), ..., (\\boldsymbol{x}^{(m)}, y^{(m)})\\}$\n",
    "- with $\\boldsymbol{x}^{(i)}$ being a $d-$dimensional vector $\\boldsymbol{x}^i = (x^{(i)}_1, ..., x^{(i)}_d)$\n",
    "- $y^{(i)}$ being a scalar target variable\n",
    "\n",
    "The linear regression model can be interpreted as a very **simple neural network:**\n",
    "- it has a real-valued weight vector $\\boldsymbol{w}= (w^{(1)}, ..., w^{(d)})$\n",
    "- it has a real-valued bias $b$\n",
    "- it uses the identity function as its activation function\n",
    "\n",
    "A linear regression model can be trained using either  \n",
    "a) gradient descent or  \n",
    "b) the normal equation (closed-form solution): $\\boldsymbol{w} = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$\n",
    "\n",
    "where $\\boldsymbol{X}$ is a matrix of shape $(m, n_{features})$ that holds all training examples.  \n",
    "The normal equation requires computing the inverse of $\\boldsymbol{X}^T \\boldsymbol{X}$. The computational complexity of this operation lies between $O(n_{features}^{2.4}$) and $O(n_{features}^3$) (depending on the implementation).\n",
    "Therefore, if the number of features in the training set is large, the normal equation will get very slow. \n",
    "\n",
    "* * *\n",
    "The derivation of linear regression using least squares,\n",
    "\n",
    "$\\boldsymbol{\\hat{y}} = \\boldsymbol{X} \\cdot \\boldsymbol{w}$\n",
    "\n",
    "$Expectation = \\boldsymbol{\\hat{y}} - \\boldsymbol{X} \\cdot \\boldsymbol{w}$\n",
    "\n",
    "Take the squared error of the expectation:\n",
    "\n",
    "$J(\\boldsymbol{w}) = ||E||^2 =  ||(\\boldsymbol{\\hat{y}} - \\boldsymbol{X} \\cdot \\boldsymbol{w})||^2$\n",
    "\n",
    "To minimize the expectation:\n",
    "\n",
    "$\\boldsymbol{\\hat{w}} = argma_w ||(\\boldsymbol{\\hat{y}} - \\boldsymbol{X} \\cdot \\boldsymbol{w})||^2$\n",
    "\n",
    "$J(\\boldsymbol{w}) = (\\boldsymbol{\\hat{y}} - \\boldsymbol{X} \\cdot \\boldsymbol{w})^T (\\boldsymbol{\\hat{y}} - \\boldsymbol{X} \\cdot \\boldsymbol{w})= \\boldsymbol{\\hat{y}}^T\\boldsymbol{\\hat{y}}  - 2 \\boldsymbol{w}^T\\boldsymbol{X}^T\\boldsymbol{\\hat{y}} + \\boldsymbol{w}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{w} $\n",
    "\n",
    "Find the derivative of $J(\\boldsymbol{w})$ and let it equals to 0.\n",
    "\n",
    "$ \\frac{\\partial J}{\\partial w_j} = - 2 \\boldsymbol{X}^T \\boldsymbol{\\hat{y}} + 2\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{w}$\n",
    "\n",
    "$ Let  - 2 \\boldsymbol{X}^T \\boldsymbol{\\hat{y}} + 2\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{w} = 0$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$\\boldsymbol{w} = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$, where $\\ (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T$ is called pseudo inverse of $\\ \\boldsymbol{X}$, shown as $\\beta$ in code.\n",
    "\n",
    "* * *\n",
    "The training procedure of a linear regression model has different steps. In the beginning (step 0) the model parameters are initialized. The other steps (see below) are repeated for a specified number of training iterations or until the parameters have converged.\n",
    "\n",
    "**Step 0: ** \n",
    "\n",
    "Initialize the weight vector and bias with zeros (or small random values)\n",
    "\n",
    "**OR**\n",
    "\n",
    "Compute the parameters directly using the normal equation\n",
    "* * *\n",
    "\n",
    "**Step 1: ** (Only needed when training with gradient descent)\n",
    "\n",
    "Compute a linear combination of the input features and weights. This can be done in one step for all training examples, using vectorization and broadcasting:\n",
    "$\\boldsymbol{\\hat{y}} = \\boldsymbol{X} \\cdot \\boldsymbol{w} + b $\n",
    "\n",
    "where $\\boldsymbol{X}$ is a matrix of shape $(m, n_{features})$ that holds all training examples, and $\\cdot$ denotes the dot product.\n",
    "* * *\n",
    "\n",
    "**Step 2: ** (Only needed when training with gradient descent)\n",
    "\n",
    "Compute the cost (mean squared error) over the training set:\n",
    "\n",
    "$J(\\boldsymbol{w},b) = \\frac{1}{m} \\sum_{i=1}^m \\Big(\\hat{y}^{(i)} - y^{(i)} \\Big)^2$\n",
    "* * *\n",
    "\n",
    "**Step 3: **  (Only needed when training with gradient descent)\n",
    "\n",
    "Compute the partial derivatives of the cost function with respect to each parameter:\n",
    "\n",
    "$ \\frac{\\partial J}{\\partial w_j} = \\frac{2}{m}\\sum_{i=1}^m \\Big( \\hat{y}^{(i)} - y^{(i)} \\Big) x^{(i)}_j$\n",
    "\n",
    "$ \\frac{\\partial J}{\\partial b} = \\frac{2}{m}\\sum_{i=1}^m \\Big( \\hat{y}^{(i)} - y^{(i)} \\Big)$\n",
    "\n",
    "\n",
    "The gradient containing all partial derivatives can then be computed as follows: \n",
    "\n",
    "$\\nabla_{\\boldsymbol{w}} J = \\frac{2}{m} \\boldsymbol{X}^T \\cdot \\big(\\boldsymbol{\\hat{y}} - \\boldsymbol{y} \\big)$\n",
    "\n",
    "$\\nabla_{\\boldsymbol{b}} J = \\frac{2}{m} \\big(\\boldsymbol{\\hat{y}} - \\boldsymbol{y} \\big)$\n",
    "* * *\n",
    "\n",
    "**Step 4: ** (Only needed when training with gradient descent)\n",
    "\n",
    "Update the weight vector and bias:\n",
    "\n",
    "$\\boldsymbol{w} = \\boldsymbol{w} - \\eta \\, \\nabla_w J$  \n",
    "\n",
    "$b = b - \\eta \\, \\nabla_b J$  \n",
    "\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}