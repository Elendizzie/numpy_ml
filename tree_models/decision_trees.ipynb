{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Decision tree in plain Python\n",
    "\n",
    "A decision tree is a **supervised** machine learning model that can be used both for **classification** and **regression**. At its core, a decision tree uses a tree structure to predict an output value for a given input example. In the tree, each path from the root node to a leaf node represents a *decision path* that ends in a predicted value. \n",
    "\n",
    "A simple classification example might look as follows:\n",
    "\n",
    "![caption](../figures/decision_tree.png)\n",
    "\n",
    "A simple regressiion example might look as follows:\n",
    "\n",
    "![caption](../figures/regression_tree.png)\n",
    "\n",
    "Decision trees have many advantages. For example, they are easy to understand and their decisions are easy to interpret. Also, they don't require a lot of data preparation. A more extensive list of their advantages and disadvantages can be found [here](http://scikit-learn.org/stable/modules/tree.html).\n",
    "\n",
    "### CART training algorithm \n",
    "In order to train a decision tree, various algorithms can be used. In this notebook we will focus on the *CART* algorithm (Classification and Regression Trees) for *classification*. The CART algorithm builds a *binary tree* in which every non-leaf node has exactly two children (corresponding to a yes/no answer). \n",
    "\n",
    "Given a set of training examples and their labels, the algorithm repeatedly splits the training examples $D$ into two subsets $D_{left}, D_{right}$ using some feature $f$ and feature threshold $t_f$ such that samples with the same label are grouped together. At each node, the algorithm selects the split $\\theta = (f, t_f)$ that produces the purest subsets, weighted by their size. Purity/impurity is measured using the *Gini impurity*.\n",
    "\n",
    "So at each step, the algorithm selects the parameters $\\theta$ that minimize the following cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "J(D, \\theta) = \\frac{n_{left}}{n_{total}} G_{left} + \\frac{n_{right}}{n_{total}} G_{right}\n",
    "\\end{equation}\n",
    "\n",
    "- $D$: remaining training examples   \n",
    "- $n_{total}$ : number of remaining training examples\n",
    "- $\\theta = (f, t_f)$: feature and feature threshold\n",
    "- $n_{left}/n_{right}$: number of samples in the left/right subset\n",
    "- $G_{left}/G_{right}$: Gini impurity of the left/right subset\n",
    "\n",
    "This step is repeated recursively until the *maximum allowable depth* is reached or the current number of samples $n_{total}$ drops below some minimum number. The original equations can be found [here](http://scikit-learn.org/stable/modules/tree.html).\n",
    "\n",
    "### For regression, use the following cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "J(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}\n",
    "\\end{equation}\n",
    "\n",
    "- $D$: remaining training examples   \n",
    "- $n_{total}$ : number of remaining training examples\n",
    "- $\\theta = (f, t_f)$: feature and feature threshold\n",
    "- $n_{left}/n_{right}$: number of samples in the left/right subset\n",
    "- $MSE_{left}/MSE_{right}$: MSE of the left/right subset\n",
    "\n",
    "After building the tree, new examples can be classified by navigating through the tree, testing at each node the corresponding feature until a leaf node/prediction is reached.\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "Given $K$ different classification values $k \\in \\{1, ..., K\\}$ the Gini impurity of node $m$ is computed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "G_m = 1 - \\sum_{k=1}^{K} (p_{m,k})^2\n",
    "\\end{equation}\n",
    "\n",
    "where $p_{m, k}$ is the fraction of training examples with class $k$ among all training examples in node $m$.\n",
    "\n",
    "The Gini impurity can be used to evaluate how good a potential split is. A split divides a given set of training examples into two groups. Gini measures how \"mixed\" the resulting groups are. A perfect separation (i.e. each group contains only samples of the same class) corresponds to a Gini impurity of 0. If the resulting groups contain equally many samples of each class, the Gini impurity will reach its highest value of 0.5\n",
    "\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "When performing regression (i.e. the target values are continuous) we can evaluate a split using its MSE. The MSE of node $m$ is computed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} y_i\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "MSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2\n",
    "\\end{equation}\n",
    "\n",
    "- $D_m$: training examples in node $m$\n",
    "- $n_{m}$ : total number of training examples in node $m$\n",
    "- $y_i$: target value of $i-$th example\n",
    "\n",
    "### Caveats\n",
    "\n",
    "Without regularization, decision trees are likely to overfit the training examples. This can be prevented using techniques like *pruning* or by providing a maximum allowed tree depth and/or a minimum number of samples required to split a node further.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
